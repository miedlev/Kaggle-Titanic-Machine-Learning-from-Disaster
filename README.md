### Kaggle Competition | [Titanic Machine Learning from Disaster](https://www.kaggle.com/c/titanic/overview/description)

This is the legendary Titanic ML competition / the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.


#### 1. Introduction & Process :
It is a competition that can be said to be Kaggle's introductory period and conducts a Python-based analysis. My focusins was on 
1. dummy variable generation
2. feature engineering
3. word-based column generation
4. continuous variables range
5. tree-based model

#### 2. Dependencies & Tech :
* [NumPy](http://www.numpy.org/)
* [IPython](http://ipython.org/)
* [Pandas](http://pandas.pydata.org/)
* [SciKit-Learn](http://scikit-learn.org/stable/)
* [SciPy](http://www.scipy.org/)
* [Seaborn](https://seaborn.pydata.org/)
* [Matplotlib](http://matplotlib.org/)
* [StatsModels](http://statsmodels.sourceforge.net/)

#### 3. Conclusion Analysis Report - Jupyter Notebook
* [TitanicFinal](https://github.com/miedlev/Kaggle-Titanic-Machine-Learning-from-Disaster/blob/master/TitanicFinal%20part.ipynb)


#### Titanic Machine Learning from Disaster
The sinking of the Titanic is one of the most infamous shipwrecks in history.

On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.

While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.

In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).

